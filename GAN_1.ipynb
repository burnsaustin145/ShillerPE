{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mediterranean-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.onnx as onnx\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "heavy-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import pprint as pp\n",
    "import xlrd\n",
    "import csv\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "copyrighted-quest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1804, 22, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def csv_from_excel():\n",
    "    wb = xlrd.open_workbook('/dat/ie_data.xls')\n",
    "    sh = wb.sheet_by_name('Data')\n",
    "    your_csv_file = open('/dat/pe_shill.csv', 'w')\n",
    "    wr = csv.writer(your_csv_file, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    for rownum in range(sh.nrows):\n",
    "        wr.writerow(sh.row_values(rownum))\n",
    "    your_csv_file.close()\n",
    "\n",
    "# shiller csv, good data has 22 elements. Still need to format labels\n",
    "def csv_to_arr(arr=[]):\n",
    "    \n",
    "    pe_array = arr\n",
    "    with open('pe_shill.csv') as file:\n",
    "        reader = csv.reader(file, quotechar='|')\n",
    "        for row in reader:\n",
    "            pe_array.append(row)\n",
    "\n",
    "    pe_array = pe_array[5:]\n",
    "    return pe_array\n",
    "\n",
    "arr = csv_to_arr() \n",
    "label_arr = arr[:3]\n",
    "arr = arr[3:]\n",
    "\n",
    "labels = []\n",
    "count = 0\n",
    "for x in label_arr:\n",
    "    \n",
    "    curr = 0\n",
    "    for y in x:\n",
    "        if len(labels) <= curr and count == 0:\n",
    "            labels.append(y.replace('\"',' '))\n",
    "            curr += 1\n",
    "        else:\n",
    "            labels[curr] = labels[curr] + y.replace('\"',' ')\n",
    "            curr += 1\n",
    "    count += 1\n",
    "\n",
    "#print(len(labels))\n",
    "#pp.pprint(labels)\n",
    "#print(arr)\n",
    "\n",
    "def clean_arr(arr):\n",
    "    cleaned = []\n",
    "    curr = 0\n",
    "    for i in arr:\n",
    "        cleaned.append([])\n",
    "        \n",
    "        for j in i:\n",
    "            \n",
    "            foo = j[1:-1]\n",
    "            try:\n",
    "                cleaned[curr].append([float(foo)])\n",
    "            except:            \n",
    "                cleaned[curr].append([0])\n",
    "        curr += 1\n",
    "    return cleaned\n",
    "\n",
    "pe_arr = clean_arr(arr)\n",
    "arr = 0\n",
    "pe_arr = np.asarray(pe_arr)\n",
    "\n",
    "x_pe = torch.from_numpy(pe_arr)\n",
    "pe_arr = 0\n",
    "x_pe = x_pe.float()\n",
    "x_pe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coral-testimony",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "class GNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNet, self).__init__()\n",
    "\n",
    "        self.ker = 1\n",
    "        self.ngf = 1804\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose1d(100, self.ngf*8, self.ker, 2, bias=False),\n",
    "            nn.BatchNorm1d(self.ngf*8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose1d(self.ngf*8, self.ngf*4, self.ker, 2, bias=False),\n",
    "            nn.BatchNorm1d(self.ngf*4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose1d(self.ngf*4, self.ngf*2, self.ker, 2, bias=False),\n",
    "            nn.BatchNorm1d(self.ngf*2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose1d(self.ngf*2, self.ngf, self.ker, 2, bias=False),\n",
    "            nn.BatchNorm1d(self.ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose1d(self.ngf, 22, self.ker, 2, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        return self.main(input)\n",
    "    \n",
    "\n",
    "class DNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNet, self).__init__()\n",
    "\n",
    "        self.ker = 1\n",
    "        self.ndf = 1804\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(22, self.ndf, self.ker, 2, bias=False), \n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "            \n",
    "            nn.Conv1d(self.ndf, self.ndf*2, self.ker, 2, bias=False),\n",
    "            nn.BatchNorm1d(self.ndf*2), \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv1d(self.ndf*2, self.ndf*4, self.ker, 2, bias=False),\n",
    "            nn.BatchNorm1d(self.ndf*4), \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv1d(self.ndf*4, self.ndf*8, self.ker, 2, bias=False),\n",
    "            nn.BatchNorm1d(self.ndf*8), \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv1d(self.ndf*8, 1, self.ker, 2, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "measured-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = GNet().to(device)\n",
    "D = DNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rural-denver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNet(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose1d(100, 14432, kernel_size=(1,), stride=(2,), bias=False)\n",
      "    (1): BatchNorm1d(14432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose1d(14432, 7216, kernel_size=(1,), stride=(2,), bias=False)\n",
      "    (4): BatchNorm1d(7216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose1d(7216, 3608, kernel_size=(1,), stride=(2,), bias=False)\n",
      "    (7): BatchNorm1d(3608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose1d(3608, 1804, kernel_size=(1,), stride=(2,), bias=False)\n",
      "    (10): BatchNorm1d(1804, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose1d(1804, 22, kernel_size=(1,), stride=(2,), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "DNet(\n",
      "  (main): Sequential(\n",
      "    (0): Conv1d(22, 1804, kernel_size=(1,), stride=(2,), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv1d(1804, 3608, kernel_size=(1,), stride=(2,), bias=False)\n",
      "    (3): BatchNorm1d(3608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv1d(3608, 7216, kernel_size=(1,), stride=(2,), bias=False)\n",
      "    (6): BatchNorm1d(7216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv1d(7216, 14432, kernel_size=(1,), stride=(2,), bias=False)\n",
      "    (9): BatchNorm1d(14432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv1d(14432, 1, kernel_size=(1,), stride=(2,), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(G)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "needed-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(100, 14432, 1, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "optimizerD = optim.Adam(D.parameters(), lr=.0002, betas=(.5, 0.999))\n",
    "optimizerG = optim.Adam(G.parameters(), lr=.0002, betas=(.5, 0.999))\n",
    "\n",
    "ds = torch.utils.data.TensorDataset(x_pe)\n",
    "x_pe = 0\n",
    "loader = DataLoader(ds, batch_size=1804, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-strengthening",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "[0/1000][0/1]\tLoss_D: 1.8980\tLoss_G: 6.8640\tD(x): 0.4534\tD(G(z)): 0.4931 / 0.5111\n"
     ]
    }
   ],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(1000):\n",
    "    for i, data in enumerate(loader, 0):\n",
    "\n",
    "        D.zero_grad()\n",
    "        \n",
    "        real = data[0].to(device)\n",
    "        b_size = real.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        output = D(real).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        \n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        # Train w/ fake\n",
    "        noise = torch.randn(b_size, 100, 1, device=device)\n",
    "        \n",
    "        fake = G(noise)\n",
    "        label.fill_(fake_label)\n",
    "        \n",
    "        output = D(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        \n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        \n",
    "        errD = errD_real + errD_fake\n",
    "        \n",
    "        optimizerD.step()\n",
    "        \n",
    "        # update G\n",
    "        G.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        \n",
    "        output = D(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, 1000, i, len(loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "torch.save(D, '.')\n",
    "torch.save(G, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 1,\n",
    "            'model_state_dict': D.state_dict(),\n",
    "            'optimizer_state_dict': optimizerD.state_dict(),\n",
    "            'loss': D_losses,\n",
    "            }, 'D.p')\n",
    "\n",
    "torch.save({\n",
    "            'epoch': 1,\n",
    "            'model_state_dict': G.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': G_losses,\n",
    "            }, 'G.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('test_1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
